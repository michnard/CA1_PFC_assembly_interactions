{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code associated with paper \"Theta oscillations as a substrate for medial prefrontal-hippocampal assembly interactions\"\n",
    "\n",
    "Michele Nardin, Karola Kaefer, Federico Stella, Jozsef Csicsvari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will load spiking data, prepare the matrix of covariates, fit GLM models with L2 penalty and cross validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stat\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripts for fitting GLM models based on statsmodels GLM Poisson family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitGLM_lam(X,     # matrix of covariates\n",
    "                Y,    # 1 X T, vector of binned spiking data\n",
    "                lam): # optimal L2 penalty found through cross-validation\n",
    "    T = len(Y)\n",
    "    g = sm.GLM(Y, X.astype(float), family=sm.families.Poisson())\n",
    "    glm=g.fit_regularized(L1_wt=0, alpha=lam)\n",
    "    return glm.params\n",
    "\n",
    "logfac = np.array([0,0]+[np.sum(np.log(np.arange(2,n))) for n in range(3,10000)])\n",
    "def loglglm(X,Y,an):\n",
    "    return np.mean(-np.exp(an@X) + Y*(an@X) - logfac[Y.astype(int)])\n",
    "\n",
    "def cross_valid_sing_cell(X,Y):\n",
    "\n",
    "    pl = [0.000001,0.00001,0.0001,0.001,0.01,0.1] # possible L2 penalties \n",
    "    llk = [] # save log-likelihoods here across 4-fold 75/25 cross validation\n",
    "    for l in pl:\n",
    "        llk.append([])\n",
    "        # 4-fold cross validation for each cell with random 75 train / 25 test\n",
    "        for jk in range(4): \n",
    "            ## randomly split train/test\n",
    "            X_train,  X_test, Y_train, Y_test = tts(X,Y,test_size=0.25,shuffle=True)\n",
    "            # fit GLM\n",
    "            g = sm.GLM(Y_train, X_train.astype(float), family=sm.families.Poisson())\n",
    "            glm=g.fit_regularized(L1_wt=0, alpha=l,cnvrg_tol=1e-05) # L2 regularize\n",
    "            # measure log-likelihood on test data\n",
    "            llk[-1].append(loglglm(X_test.T,Y_test,glm.params))\n",
    "    best_lam = pl[np.argmax(np.mean(llk,1))] # find best lambda\n",
    "    return best_lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load covariates (1-hot for position, trajectory, experimental phase, speed, theta)\n",
    "covariates = np.load('data/covariates.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29908, 424)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check size\n",
    "covariates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load firing\n",
    "fir_CA1 = np.load('data/fir_CA1.npy')\n",
    "fir_PFC = np.load('data/fir_PFC.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29908, 46), (29908, 42))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check sizes\n",
    "fir_CA1.shape,fir_PFC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA1\n",
      "**********************************************\n",
      " PFC\n",
      "******************************************"
     ]
    }
   ],
   "source": [
    "# fit GLM models on single cells data\n",
    "# find optimal L2 penalty through cross-validation\n",
    "\n",
    "# CA1\n",
    "T, N = fir_CA1.shape\n",
    "print('CA1')\n",
    "CA1_glm_params = [] # parameters\n",
    "CA1_glm_firing = [] # GLM expected firing rates\n",
    "for cell in range(N):\n",
    "    print('*',end='')\n",
    "    Y = fir_CA1[:,cell]\n",
    "    # prepare X: \n",
    "    # firing of other cells within the same area\n",
    "    others = np.delete(fir_CA1,cell,axis=1)\n",
    "    # firing history\n",
    "    hist = np.concatenate([[0],Y[:-1]]).reshape([-1,1])\n",
    "    # stack with other covariates (position, trajectory, exp phase, speed, theta)\n",
    "    X = np.hstack([covariates, hist,others])\n",
    "    # fit GLM model using cross-validation\n",
    "    lam = cross_valid_sing_cell(X,Y)\n",
    "    # using the best lambda, fit the model on the full data\n",
    "    params = fitGLM_lam(X,Y,lam)\n",
    "    # store parameters in a vector, to save\n",
    "    CA1_glm_params.append(params)\n",
    "    # store GLM expected firing rate\n",
    "    CA1_glm_firing.append(np.exp(X@params))\n",
    "    \n",
    "# PFC\n",
    "T, N = fir_PFC.shape\n",
    "print('\\n PFC')\n",
    "PFC_glm_params = []\n",
    "PFC_glm_firing = []\n",
    "for cell in range(N):\n",
    "    Y = fir_PFC[:,cell]\n",
    "    print('*',end='')\n",
    "    # prepare X:\n",
    "    # firing of other cells\n",
    "    others = np.delete(fir_PFC,cell,axis=1)\n",
    "    # firing history of the neuron of interest\n",
    "    hist = np.concatenate([[0],Y[:-1]]).reshape([-1,1])\n",
    "    # and stack with other covariates (position, trajectory, experimental phase, speed, theta)\n",
    "    X = np.hstack([covariates, hist,others])\n",
    "    # find optimal lambda using cross-validation\n",
    "    lam = cross_valid_sing_cell(X,Y)\n",
    "    # using the best lambda, fit the model on the full data\n",
    "    params = fitGLM_lam(X,Y,lam)\n",
    "    # store parameters in a vector, to save\n",
    "    PFC_glm_params.append(params)\n",
    "    # store GLM expected firing rate\n",
    "    PFC_glm_firing.append(np.exp(X@params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GLM parameters\n",
    "np.save('fits/CA1_glm_params',CA1_glm_params)\n",
    "np.save('fits/PFC_glm_params',PFC_glm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also save expected firing rates - we're gonna need them later to infer noise correlations\n",
    "np.save('fits/CA1_glm_firing',CA1_glm_firing)\n",
    "np.save('fits/PFC_glm_firing',PFC_glm_firing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
